{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import config\n",
    "config.GPU =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.config import GPU\n",
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m------------------------------------------------------------\u001b[0m\n",
      "                       \u001b[92mGPU Mode (cupy)\u001b[0m\n",
      "\u001b[92m------------------------------------------------------------\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if GPU: # GPU\n",
    "    import cupy as np\n",
    "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
    "    #np.add.at = np.scatter_add\n",
    "\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
    "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
    "\n",
    "\n",
    "# 4.3.1 CBOW 모델 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[np.asnumpy(idx)] # numpy 일텐데 idx가 cupy.core~ array라서\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.scatter_add(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# from common.np import *  # import numpy as np\n",
    "#from common.layers import SigmoidWithLoss\n",
    "import collections\n",
    "\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n",
    "\n",
    "\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = np.asnumpy(target[i])\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n",
    "            # 부정적 예에 타깃이 포함될 수 있다.\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample\n",
    "\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 긍정적 예 순전파\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 부정적 예 순전파\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('D:/ANACONDA/envs/tf-gpu/code/NLP')\n",
    "#import cupy as np\n",
    "#from common.layers import Embedding\n",
    "#from common.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size,hidden_size,window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V,H).astype('f')\n",
    "        W_out = 0.01 * np.random.rand(V,H).astype('f')\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out,corpus, power=0.75, sample_size=5)\n",
    "        \n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        #인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "    \n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:,i])\n",
    "        h *= 1/ len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h,target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None\n",
    "    \n",
    "# Skip-Gram\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * rn(V, H).astype('f')\n",
    "        W_out = 0.01 * rn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer = Embedding(W_in)\n",
    "        self.loss_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "            self.loss_layers.append(layer)\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer] + self.loss_layers\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "\n",
    "        loss = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            loss += layer.forward(h, contexts[:, i])\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            dh += layer.backward(dout)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common.trainer import Trainer\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.util import clip_grads\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=500):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('..')\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.2 CBOW 모델 학습 코드\n",
    "\n",
    "#from common import config\n",
    "#config.GPU = True # pip install cupy-cuda92  -----> for windows 10 # 위에서 했고\n",
    "import pickle\n",
    "#from common.trainer import Trainer\n",
    "#from common.optimizer import Adam\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "#from cbow import CBOW\n",
    "#from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "\n",
    "def to_cpu(x):\n",
    "    import numpy\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return np.asnumpy(x)\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n",
    "\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기 + target, contexts 만들기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 손실 4.16\n",
      "| 에폭 1 |  반복 501 / 9295 | 시간 8[s] | 손실 3.17\n",
      "| 에폭 1 |  반복 1001 / 9295 | 시간 15[s] | 손실 2.62\n",
      "| 에폭 1 |  반복 1501 / 9295 | 시간 23[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 2001 / 9295 | 시간 31[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 2501 / 9295 | 시간 40[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 3001 / 9295 | 시간 47[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 3501 / 9295 | 시간 56[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 4001 / 9295 | 시간 64[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 4501 / 9295 | 시간 72[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 5001 / 9295 | 시간 80[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 5501 / 9295 | 시간 88[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 6001 / 9295 | 시간 96[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 6501 / 9295 | 시간 104[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 7001 / 9295 | 시간 112[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 7501 / 9295 | 시간 120[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 8001 / 9295 | 시간 128[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 8501 / 9295 | 시간 136[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 9001 / 9295 | 시간 144[s] | 손실 2.28\n",
      "| 에폭 2 |  반복 1 / 9295 | 시간 148[s] | 손실 2.27\n",
      "| 에폭 2 |  반복 501 / 9295 | 시간 157[s] | 손실 2.22\n",
      "| 에폭 2 |  반복 1001 / 9295 | 시간 165[s] | 손실 2.21\n",
      "| 에폭 2 |  반복 1501 / 9295 | 시간 173[s] | 손실 2.20\n",
      "| 에폭 2 |  반복 2001 / 9295 | 시간 181[s] | 손실 2.19\n",
      "| 에폭 2 |  반복 2501 / 9295 | 시간 189[s] | 손실 2.18\n",
      "| 에폭 2 |  반복 3001 / 9295 | 시간 196[s] | 손실 2.18\n",
      "| 에폭 2 |  반복 3501 / 9295 | 시간 204[s] | 손실 2.16\n",
      "| 에폭 2 |  반복 4001 / 9295 | 시간 212[s] | 손실 2.15\n",
      "| 에폭 2 |  반복 4501 / 9295 | 시간 221[s] | 손실 2.14\n",
      "| 에폭 2 |  반복 5001 / 9295 | 시간 228[s] | 손실 2.13\n",
      "| 에폭 2 |  반복 5501 / 9295 | 시간 236[s] | 손실 2.13\n",
      "| 에폭 2 |  반복 6001 / 9295 | 시간 244[s] | 손실 2.12\n",
      "| 에폭 2 |  반복 6501 / 9295 | 시간 252[s] | 손실 2.11\n",
      "| 에폭 2 |  반복 7001 / 9295 | 시간 260[s] | 손실 2.10\n",
      "| 에폭 2 |  반복 7501 / 9295 | 시간 268[s] | 손실 2.09\n",
      "| 에폭 2 |  반복 8001 / 9295 | 시간 276[s] | 손실 2.10\n",
      "| 에폭 2 |  반복 8501 / 9295 | 시간 284[s] | 손실 2.08\n",
      "| 에폭 2 |  반복 9001 / 9295 | 시간 292[s] | 손실 2.07\n",
      "| 에폭 3 |  반복 1 / 9295 | 시간 296[s] | 손실 2.06\n",
      "| 에폭 3 |  반복 501 / 9295 | 시간 304[s] | 손실 2.01\n",
      "| 에폭 3 |  반복 1001 / 9295 | 시간 312[s] | 손실 2.00\n",
      "| 에폭 3 |  반복 1501 / 9295 | 시간 320[s] | 손실 2.00\n",
      "| 에폭 3 |  반복 2001 / 9295 | 시간 328[s] | 손실 2.00\n",
      "| 에폭 3 |  반복 2501 / 9295 | 시간 335[s] | 손실 1.99\n",
      "| 에폭 3 |  반복 3001 / 9295 | 시간 343[s] | 손실 2.00\n",
      "| 에폭 3 |  반복 3501 / 9295 | 시간 351[s] | 손실 1.99\n",
      "| 에폭 3 |  반복 4001 / 9295 | 시간 359[s] | 손실 1.98\n",
      "| 에폭 3 |  반복 4501 / 9295 | 시간 367[s] | 손실 1.98\n",
      "| 에폭 3 |  반복 5001 / 9295 | 시간 375[s] | 손실 1.98\n",
      "| 에폭 3 |  반복 5501 / 9295 | 시간 382[s] | 손실 1.97\n",
      "| 에폭 3 |  반복 6001 / 9295 | 시간 390[s] | 손실 1.97\n",
      "| 에폭 3 |  반복 6501 / 9295 | 시간 398[s] | 손실 1.97\n",
      "| 에폭 3 |  반복 7001 / 9295 | 시간 406[s] | 손실 1.96\n",
      "| 에폭 3 |  반복 7501 / 9295 | 시간 414[s] | 손실 1.95\n",
      "| 에폭 3 |  반복 8001 / 9295 | 시간 422[s] | 손실 1.96\n",
      "| 에폭 3 |  반복 8501 / 9295 | 시간 430[s] | 손실 1.95\n",
      "| 에폭 3 |  반복 9001 / 9295 | 시간 438[s] | 손실 1.94\n",
      "| 에폭 4 |  반복 1 / 9295 | 시간 442[s] | 손실 1.94\n",
      "| 에폭 4 |  반복 501 / 9295 | 시간 450[s] | 손실 1.88\n",
      "| 에폭 4 |  반복 1001 / 9295 | 시간 458[s] | 손실 1.88\n",
      "| 에폭 4 |  반복 1501 / 9295 | 시간 466[s] | 손실 1.88\n",
      "| 에폭 4 |  반복 2001 / 9295 | 시간 474[s] | 손실 1.88\n",
      "| 에폭 4 |  반복 2501 / 9295 | 시간 481[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 3001 / 9295 | 시간 489[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 3501 / 9295 | 시간 497[s] | 손실 1.86\n",
      "| 에폭 4 |  반복 4001 / 9295 | 시간 505[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 4501 / 9295 | 시간 513[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 5001 / 9295 | 시간 521[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 5501 / 9295 | 시간 528[s] | 손실 1.86\n",
      "| 에폭 4 |  반복 6001 / 9295 | 시간 536[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 6501 / 9295 | 시간 544[s] | 손실 1.86\n",
      "| 에폭 4 |  반복 7001 / 9295 | 시간 552[s] | 손실 1.86\n",
      "| 에폭 4 |  반복 7501 / 9295 | 시간 559[s] | 손실 1.85\n",
      "| 에폭 4 |  반복 8001 / 9295 | 시간 567[s] | 손실 1.85\n",
      "| 에폭 4 |  반복 8501 / 9295 | 시간 575[s] | 손실 1.85\n",
      "| 에폭 4 |  반복 9001 / 9295 | 시간 583[s] | 손실 1.84\n",
      "| 에폭 5 |  반복 1 / 9295 | 시간 588[s] | 손실 1.85\n",
      "| 에폭 5 |  반복 501 / 9295 | 시간 596[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 1001 / 9295 | 시간 604[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 1501 / 9295 | 시간 611[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 2001 / 9295 | 시간 620[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 2501 / 9295 | 시간 628[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 3001 / 9295 | 시간 635[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 3501 / 9295 | 시간 643[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 4001 / 9295 | 시간 651[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 4501 / 9295 | 시간 659[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 5001 / 9295 | 시간 667[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 5501 / 9295 | 시간 675[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 6001 / 9295 | 시간 683[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 6501 / 9295 | 시간 691[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 7001 / 9295 | 시간 699[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 7501 / 9295 | 시간 707[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 8001 / 9295 | 시간 714[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 8501 / 9295 | 시간 722[s] | 손실 1.77\n",
      "| 에폭 5 |  반복 9001 / 9295 | 시간 730[s] | 손실 1.77\n",
      "| 에폭 6 |  반복 1 / 9295 | 시간 735[s] | 손실 1.77\n",
      "| 에폭 6 |  반복 501 / 9295 | 시간 743[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 1001 / 9295 | 시간 751[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 1501 / 9295 | 시간 759[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 2001 / 9295 | 시간 766[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 2501 / 9295 | 시간 774[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 3001 / 9295 | 시간 782[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 3501 / 9295 | 시간 790[s] | 손실 1.71\n",
      "| 에폭 6 |  반복 4001 / 9295 | 시간 798[s] | 손실 1.71\n",
      "| 에폭 6 |  반복 4501 / 9295 | 시간 806[s] | 손실 1.69\n",
      "| 에폭 6 |  반복 5001 / 9295 | 시간 814[s] | 손실 1.71\n",
      "| 에폭 6 |  반복 5501 / 9295 | 시간 822[s] | 손실 1.71\n",
      "| 에폭 6 |  반복 6001 / 9295 | 시간 830[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 6501 / 9295 | 시간 838[s] | 손실 1.71\n",
      "| 에폭 6 |  반복 7001 / 9295 | 시간 846[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 7501 / 9295 | 시간 854[s] | 손실 1.71\n",
      "| 에폭 6 |  반복 8001 / 9295 | 시간 861[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 8501 / 9295 | 시간 869[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 9001 / 9295 | 시간 878[s] | 손실 1.70\n",
      "| 에폭 7 |  반복 1 / 9295 | 시간 882[s] | 손실 1.70\n",
      "| 에폭 7 |  반복 501 / 9295 | 시간 890[s] | 손실 1.62\n",
      "| 에폭 7 |  반복 1001 / 9295 | 시간 898[s] | 손실 1.63\n",
      "| 에폭 7 |  반복 1501 / 9295 | 시간 906[s] | 손실 1.62\n",
      "| 에폭 7 |  반복 2001 / 9295 | 시간 914[s] | 손실 1.63\n",
      "| 에폭 7 |  반복 2501 / 9295 | 시간 922[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 3001 / 9295 | 시간 929[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 3501 / 9295 | 시간 937[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 4001 / 9295 | 시간 945[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 4501 / 9295 | 시간 954[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 5001 / 9295 | 시간 961[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 5501 / 9295 | 시간 970[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 6001 / 9295 | 시간 978[s] | 손실 1.65\n",
      "| 에폭 7 |  반복 6501 / 9295 | 시간 985[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 7001 / 9295 | 시간 993[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 7501 / 9295 | 시간 1001[s] | 손실 1.65\n",
      "| 에폭 7 |  반복 8001 / 9295 | 시간 1009[s] | 손실 1.65\n",
      "| 에폭 7 |  반복 8501 / 9295 | 시간 1017[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 9001 / 9295 | 시간 1025[s] | 손실 1.64\n",
      "| 에폭 8 |  반복 1 / 9295 | 시간 1030[s] | 손실 1.64\n",
      "| 에폭 8 |  반복 501 / 9295 | 시간 1038[s] | 손실 1.56\n",
      "| 에폭 8 |  반복 1001 / 9295 | 시간 1046[s] | 손실 1.57\n",
      "| 에폭 8 |  반복 1501 / 9295 | 시간 1054[s] | 손실 1.57\n",
      "| 에폭 8 |  반복 2001 / 9295 | 시간 1063[s] | 손실 1.58\n",
      "| 에폭 8 |  반복 2501 / 9295 | 시간 1070[s] | 손실 1.58\n",
      "| 에폭 8 |  반복 3001 / 9295 | 시간 1078[s] | 손실 1.58\n",
      "| 에폭 8 |  반복 3501 / 9295 | 시간 1086[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 4001 / 9295 | 시간 1094[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 4501 / 9295 | 시간 1102[s] | 손실 1.58\n",
      "| 에폭 8 |  반복 5001 / 9295 | 시간 1110[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 5501 / 9295 | 시간 1117[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 6001 / 9295 | 시간 1125[s] | 손실 1.58\n",
      "| 에폭 8 |  반복 6501 / 9295 | 시간 1133[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 7001 / 9295 | 시간 1141[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 7501 / 9295 | 시간 1149[s] | 손실 1.60\n",
      "| 에폭 8 |  반복 8001 / 9295 | 시간 1157[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 8501 / 9295 | 시간 1165[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 9001 / 9295 | 시간 1173[s] | 손실 1.60\n",
      "| 에폭 9 |  반복 1 / 9295 | 시간 1178[s] | 손실 1.60\n",
      "| 에폭 9 |  반복 501 / 9295 | 시간 1186[s] | 손실 1.52\n",
      "| 에폭 9 |  반복 1001 / 9295 | 시간 1194[s] | 손실 1.52\n",
      "| 에폭 9 |  반복 1501 / 9295 | 시간 1202[s] | 손실 1.53\n",
      "| 에폭 9 |  반복 2001 / 9295 | 시간 1210[s] | 손실 1.53\n",
      "| 에폭 9 |  반복 2501 / 9295 | 시간 1217[s] | 손실 1.53\n",
      "| 에폭 9 |  반복 3001 / 9295 | 시간 1225[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 3501 / 9295 | 시간 1233[s] | 손실 1.53\n",
      "| 에폭 9 |  반복 4001 / 9295 | 시간 1241[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 4501 / 9295 | 시간 1249[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 5001 / 9295 | 시간 1257[s] | 손실 1.55\n",
      "| 에폭 9 |  반복 5501 / 9295 | 시간 1265[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 6001 / 9295 | 시간 1273[s] | 손실 1.55\n",
      "| 에폭 9 |  반복 6501 / 9295 | 시간 1280[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 7001 / 9295 | 시간 1289[s] | 손실 1.55\n",
      "| 에폭 9 |  반복 7501 / 9295 | 시간 1297[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 8001 / 9295 | 시간 1305[s] | 손실 1.55\n",
      "| 에폭 9 |  반복 8501 / 9295 | 시간 1313[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 9001 / 9295 | 시간 1321[s] | 손실 1.55\n",
      "| 에폭 10 |  반복 1 / 9295 | 시간 1325[s] | 손실 1.55\n",
      "| 에폭 10 |  반복 501 / 9295 | 시간 1333[s] | 손실 1.48\n",
      "| 에폭 10 |  반복 1001 / 9295 | 시간 1341[s] | 손실 1.48\n",
      "| 에폭 10 |  반복 1501 / 9295 | 시간 1349[s] | 손실 1.47\n",
      "| 에폭 10 |  반복 2001 / 9295 | 시간 1357[s] | 손실 1.48\n",
      "| 에폭 10 |  반복 2501 / 9295 | 시간 1364[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 3001 / 9295 | 시간 1372[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 3501 / 9295 | 시간 1380[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 4001 / 9295 | 시간 1388[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 4501 / 9295 | 시간 1396[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 5001 / 9295 | 시간 1404[s] | 손실 1.50\n",
      "| 에폭 10 |  반복 5501 / 9295 | 시간 1412[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 6001 / 9295 | 시간 1421[s] | 손실 1.50\n",
      "| 에폭 10 |  반복 6501 / 9295 | 시간 1429[s] | 손실 1.50\n",
      "| 에폭 10 |  반복 7001 / 9295 | 시간 1436[s] | 손실 1.50\n",
      "| 에폭 10 |  반복 7501 / 9295 | 시간 1444[s] | 손실 1.50\n",
      "| 에폭 10 |  반복 8001 / 9295 | 시간 1452[s] | 손실 1.50\n",
      "| 에폭 10 |  반복 8501 / 9295 | 시간 1460[s] | 손실 1.51\n",
      "| 에폭 10 |  반복 9001 / 9295 | 시간 1468[s] | 손실 1.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size) # eval_interval=500\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = model.word_vecs\n",
    "if config.GPU :\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "with open(pkl_file,'wb') as f:\n",
    "    pickle.dump(params,f,-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.75537109375\n",
      " i: 0.74560546875\n",
      " they: 0.61572265625\n",
      " your: 0.6083984375\n",
      " anything: 0.59912109375\n",
      "\n",
      "[query] year\n",
      " month: 0.8330078125\n",
      " summer: 0.76904296875\n",
      " week: 0.7529296875\n",
      " spring: 0.75\n",
      " decade: 0.70654296875\n",
      "\n",
      "[query] car\n",
      " truck: 0.6533203125\n",
      " luxury: 0.60400390625\n",
      " window: 0.59423828125\n",
      " auto: 0.5908203125\n",
      " merkur: 0.5771484375\n",
      "\n",
      "[query] toyota\n",
      " honda: 0.67333984375\n",
      " marathon: 0.65234375\n",
      " supermarkets: 0.6357421875\n",
      " seita: 0.6357421875\n",
      " chevrolet: 0.6298828125\n"
     ]
    }
   ],
   "source": [
    "# 4.3.3 CBOW 모델 평가\n",
    "# GPU -> CPU, cupy -> numpy 로\n",
    "#from common.util import most_similar, analogy\n",
    "import numpy as np\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "pkl_file = './cbow_params.pkl'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    \n",
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']\n",
    "id_to_word = params['id_to_word']\n",
    "\n",
    "# 가장 비슷한(most similar) 단어 뽑기\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 4.97265625\n",
      " yard: 4.78515625\n",
      " hacker: 4.7734375\n",
      " carolinas: 4.76171875\n",
      " downside: 4.73828125\n",
      "\n",
      "[analogy] take:took = go:?\n",
      " eurodollars: 6.15625\n",
      " 're: 4.51953125\n",
      " came: 4.4765625\n",
      " went: 4.46875\n",
      " were: 4.44140625\n",
      "\n",
      "[analogy] car:cars = child:?\n",
      " a.m: 7.0234375\n",
      " daffynition: 5.94140625\n",
      " rape: 5.72265625\n",
      " women: 5.27734375\n",
      " incest: 5.24609375\n",
      "\n",
      "[analogy] good:better = bad:?\n",
      " rather: 5.5546875\n",
      " more: 5.55078125\n",
      " less: 5.4140625\n",
      " greater: 4.34375\n",
      " worse: 3.744140625\n"
     ]
    }
   ],
   "source": [
    "# 유추(analogy) 작업\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 손실 41.59\n",
      "| 에폭 1 |  반복 2001 / 9295 | 시간 217[s] | 손실 28.18\n",
      "| 에폭 1 |  반복 4001 / 9295 | 시간 433[s] | 손실 25.10\n",
      "| 에폭 1 |  반복 6001 / 9295 | 시간 650[s] | 손실 24.64\n",
      "| 에폭 1 |  반복 8001 / 9295 | 시간 867[s] | 손실 24.41\n",
      "| 에폭 2 |  반복 1 / 9295 | 시간 1007[s] | 손실 24.28\n",
      "| 에폭 2 |  반복 2001 / 9295 | 시간 1222[s] | 손실 24.07\n",
      "| 에폭 2 |  반복 4001 / 9295 | 시간 1438[s] | 손실 24.02\n",
      "| 에폭 2 |  반복 6001 / 9295 | 시간 1654[s] | 손실 23.95\n",
      "| 에폭 2 |  반복 8001 / 9295 | 시간 1869[s] | 손실 23.90\n",
      "| 에폭 3 |  반복 1 / 9295 | 시간 2009[s] | 손실 23.85\n",
      "| 에폭 3 |  반복 2001 / 9295 | 시간 2224[s] | 손실 23.61\n",
      "| 에폭 3 |  반복 4001 / 9295 | 시간 2439[s] | 손실 23.61\n",
      "| 에폭 3 |  반복 6001 / 9295 | 시간 2653[s] | 손실 23.60\n",
      "| 에폭 3 |  반복 8001 / 9295 | 시간 2870[s] | 손실 23.59\n",
      "| 에폭 4 |  반복 1 / 9295 | 시간 3008[s] | 손실 23.58\n",
      "| 에폭 4 |  반복 2001 / 9295 | 시간 3224[s] | 손실 23.30\n",
      "| 에폭 4 |  반복 4001 / 9295 | 시간 3438[s] | 손실 23.34\n",
      "| 에폭 4 |  반복 6001 / 9295 | 시간 3651[s] | 손실 23.35\n",
      "| 에폭 4 |  반복 8001 / 9295 | 시간 3864[s] | 손실 23.37\n",
      "| 에폭 5 |  반복 1 / 9295 | 시간 4002[s] | 손실 23.37\n",
      "| 에폭 5 |  반복 2001 / 9295 | 시간 4216[s] | 손실 23.08\n",
      "| 에폭 5 |  반복 4001 / 9295 | 시간 4429[s] | 손실 23.15\n",
      "| 에폭 5 |  반복 6001 / 9295 | 시간 4643[s] | 손실 23.18\n",
      "| 에폭 5 |  반복 8001 / 9295 | 시간 4857[s] | 손실 23.20\n",
      "| 에폭 6 |  반복 1 / 9295 | 시간 4996[s] | 손실 23.21\n",
      "| 에폭 6 |  반복 2001 / 9295 | 시간 5208[s] | 손실 22.92\n",
      "| 에폭 6 |  반복 4001 / 9295 | 시간 5421[s] | 손실 22.99\n",
      "| 에폭 6 |  반복 6001 / 9295 | 시간 5635[s] | 손실 23.03\n",
      "| 에폭 6 |  반복 8001 / 9295 | 시간 5850[s] | 손실 23.07\n",
      "| 에폭 7 |  반복 1 / 9295 | 시간 5987[s] | 손실 23.09\n",
      "| 에폭 7 |  반복 2001 / 9295 | 시간 6202[s] | 손실 22.80\n",
      "| 에폭 7 |  반복 4001 / 9295 | 시간 6415[s] | 손실 22.87\n",
      "| 에폭 7 |  반복 6001 / 9295 | 시간 6629[s] | 손실 22.92\n",
      "| 에폭 7 |  반복 8001 / 9295 | 시간 6842[s] | 손실 22.98\n",
      "| 에폭 8 |  반복 1 / 9295 | 시간 6981[s] | 손실 23.00\n",
      "| 에폭 8 |  반복 2001 / 9295 | 시간 7194[s] | 손실 22.69\n",
      "| 에폭 8 |  반복 4001 / 9295 | 시간 7409[s] | 손실 22.78\n",
      "| 에폭 8 |  반복 6001 / 9295 | 시간 7622[s] | 손실 22.84\n",
      "| 에폭 8 |  반복 8001 / 9295 | 시간 7836[s] | 손실 22.90\n",
      "| 에폭 9 |  반복 1 / 9295 | 시간 7975[s] | 손실 22.92\n",
      "| 에폭 9 |  반복 2001 / 9295 | 시간 8189[s] | 손실 22.62\n",
      "| 에폭 9 |  반복 4001 / 9295 | 시간 8402[s] | 손실 22.70\n",
      "| 에폭 9 |  반복 6001 / 9295 | 시간 8615[s] | 손실 22.78\n",
      "| 에폭 9 |  반복 8001 / 9295 | 시간 8828[s] | 손실 22.83\n",
      "| 에폭 10 |  반복 1 / 9295 | 시간 8966[s] | 손실 22.87\n",
      "| 에폭 10 |  반복 2001 / 9295 | 시간 9179[s] | 손실 22.56\n",
      "| 에폭 10 |  반복 4001 / 9295 | 시간 9393[s] | 손실 22.65\n",
      "| 에폭 10 |  반복 6001 / 9295 | 시간 9607[s] | 손실 22.72\n",
      "| 에폭 10 |  반복 8001 / 9295 | 시간 9820[s] | 손실 22.78\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 등 생성\n",
    "model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size,eval_interval=2000)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = model.word_vecs\n",
    "if config.GPU :\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'skipgram_params.pkl'\n",
    "with open(pkl_file,'wb') as f:\n",
    "    pickle.dump(params,f,-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " i: 0.67236328125\n",
      " your: 0.64990234375\n",
      " yourself: 0.642578125\n",
      " weird: 0.62939453125\n",
      " we: 0.61279296875\n",
      "\n",
      "[query] year\n",
      " month: 0.6357421875\n",
      " earlier: 0.61572265625\n",
      " week: 0.5498046875\n",
      " summer: 0.505859375\n",
      " tons: 0.50048828125\n",
      "\n",
      "[query] car\n",
      " luxury: 0.60986328125\n",
      " merkur: 0.595703125\n",
      " auto: 0.56396484375\n",
      " cars: 0.56298828125\n",
      " mazda: 0.552734375\n",
      "\n",
      "[query] toyota\n",
      " lexus: 0.68115234375\n",
      " motor: 0.6787109375\n",
      " infiniti: 0.65771484375\n",
      " honda: 0.63330078125\n",
      " beretta: 0.58984375\n",
      "--------------------------------------------------\n",
      "\n",
      "[analogy] king:man = queen:?\n",
      " candlestick: 1.71484375\n",
      " bikers: 1.7080078125\n",
      " yard: 1.69921875\n",
      " ghost: 1.673828125\n",
      " artist: 1.6669921875\n",
      "\n",
      "[analogy] take:took = go:?\n",
      " pricings: 2.013671875\n",
      " ran: 1.669921875\n",
      " feet: 1.64453125\n",
      " runs: 1.580078125\n",
      " stands: 1.55078125\n",
      "\n",
      "[analogy] car:cars = child:?\n",
      " adults: 2.26171875\n",
      " rape: 2.1953125\n",
      " incest: 1.8408203125\n",
      " districts: 1.775390625\n",
      " priced: 1.763671875\n",
      "\n",
      "[analogy] good:better = bad:?\n",
      " comparable: 1.556640625\n",
      " harm: 1.517578125\n",
      " vary: 1.46484375\n",
      " impressive: 1.3984375\n",
      " splitting: 1.37890625\n"
     ]
    }
   ],
   "source": [
    "# 4.3.3 CBOW 모델 평가\n",
    "# GPU -> CPU, cupy -> numpy 로\n",
    "#from common.util import most_similar, analogy\n",
    "import numpy as np\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n",
    "\n",
    "pkl_file = './skipgram_params.pkl'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    \n",
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']\n",
    "id_to_word = params['id_to_word']\n",
    "\n",
    "# 가장 비슷한(most similar) 단어 뽑기\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
    "\n",
    "# 유추(analogy) 작업\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.3 CBOW 모델 평가\n",
    "# GPU -> CPU, cupy -> numpy 로\n",
    "#from common.util import most_similar, analogy\n",
    "import numpy as np\n",
    "\n",
    "pkl_file = './skipgram_params.pkl'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    \n",
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']\n",
    "id_to_word = params['id_to_word']\n",
    "\n",
    "# 가장 비슷한(most similar) 단어 뽑기\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
    "\n",
    "# 유추(analogy) 작업\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
