{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. word2vec 속도 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m------------------------------------------------------------\u001b[0m\n",
      "                       \u001b[92mGPU Mode (cupy)\u001b[0m\n",
      "\u001b[92m------------------------------------------------------------\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "GPU = True\n",
    "if GPU: # GPU\n",
    "    import cupy as np\n",
    "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
    "    #np.add.at = np.scatter_add\n",
    "\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
    "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
    "else :\n",
    "    import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.1 word2vec 개선 1\n",
    "    4.1.1 Embedding 계층\n",
    "    4.1.2 Embedding 계층 구현\n",
    "    \n",
    "4.2 word2vec 개선 2\n",
    "    4.2.1 은닉층 이후 계산의 문제점\n",
    "    4.2.2 다중 분류에서 이진 분류로\n",
    "    4.2.3 시그모이드 함수와 교차 엔트로피 오차\n",
    "    4.2.4 다중 분류에서 이진 분류로 구현\n",
    "    4.2.5 네거티브 샘플링\n",
    "    4.2.6 네거티브 샘플링의 샘플링 기법\n",
    "    4.2.7 네거티브 샘플링 구현\n",
    "\n",
    "4.3 개선판 word2vec 학습\n",
    "    4.3.1 CBOW 모델 구현\n",
    "    4.3.2 CBOW 모델 학습 코드\n",
    "    4.3.3 CBOW 모델 평가\n",
    "\n",
    "4.4 word2vec 남은 주제\n",
    "    4.4.1 word2vec을 사용한 애플리케이션의 예\n",
    "    4.4.2 단어 벡터 평가 방법\n",
    "\n",
    "4.5 정리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 word2vec 개선 1\n",
    "\n",
    "[CBOW 그림]\n",
    "\n",
    "문제점 2가지 : 예를들어 어휘수가 100만개라고 한다면 상당한 메모리와 계산량이 필요하다\n",
    "- 입력층의 원핫 표현과 가중치 행렬 W_in 의 곱 계산 : 4.1 임베딩 계층 구현으로 해결\n",
    "- 은닉층과 가중치 행렬 W_out의 곱 및 Softmax 계층의 계산 : 4.2 네거티브 샘플링으로 해결\n",
    "\n",
    "4.1에서는 먼저 Embedding 계층 구현부터 살펴본다.\n",
    "\n",
    "    4.1.1 Embedding 계층\n",
    "    [그림 4-1-1]\n",
    "    : 실제 단어의 원핫 표현과 W_in의 가중치 매트릭스간의 곱의 결과는 W_in[원핫 표현에서 해당 단어의 index 번째 행] 과 동일하다는 것을 알 수 있다.\n",
    "\n",
    "    4.1.2 Embedding 계층 구현\n",
    "    - class Embedding\n",
    "    : forward 때는 index만 넘겨 은닉층(h)를 얻고\n",
    "    [그림 4-1-2]\n",
    "    : backward 때는 dW 업데이트 해야할 index가 겹치는 경우가 있을 수 있으므로 '할당'이 아닌 '더하기'를 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1.2 Embedding 계층을 구현을 이해하기 위한 기본적인 행렬 연산들\n",
    "#import numpy as np\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[2] :  [6 7 8]\n",
      "W[5] :  [15 16 17]\n",
      "W[[1,0,3,0]] = W[idx] : \n",
      " [[ 3  4  5]\n",
      " [ 0  1  2]\n",
      " [ 9 10 11]\n",
      " [ 0  1  2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W[2] : \", W[2])\n",
    "print(\"W[5] : \",W[5])\n",
    "idx = np.array([1,0,3,0])\n",
    "print(\"W[[1,0,3,0]] = W[idx] : \\n\",W[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2 Embedding 계층 구현\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx] \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.scatter_add(dW, self.idx, dout) # cpu는 numpy 일때, np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Warning]\n",
    "가중치 W와 크기가 같은 행렬 dW를 만들고 W에 더해주는 식으로 적용하려고 backward를 dW[...]=0으로 구현함 (Optimizer) 클래스와 조합해 사용하고자.\n",
    "하지만 비효율적.바로 W에 dout을 빼주면 되니까. # Optimizer 부분 봐서 수정해볼 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 word2vec 개선 2\n",
    "\n",
    "은닉층까지의 계산은 index를 활용해 필요한 계산만 실시하므로 효율적으로 바꿨다고 할 수 있으나, 여전히 W_out 부분의 계산과 수 많은 어휘와 Loss를 구해야 하는 상황.\n",
    "이를 해결하기 위해 `네거티브 샘플링`을 이용하고자 한다.\n",
    "\n",
    "    4.2.1 은닉층 이후 계산의 문제점\n",
    "    - 은닉층과 W_out (은닉층의 뉴런수 x 어휘수)간의 행렬곱은 여전히 큰 계산량\n",
    "    - Softmax 계층 계산 역시 동일 : k번째 원소를 target으로 했을때 우리가 구해야하는 값 y_k = exp(s_k) / sum(exp^(s_i)) : 결국 어휘수(예를들어 100만개라면 백만번)만큼 계산 필요.\n",
    "    \n",
    "    4.2.2 다중 분류에서 이진 분류로\n",
    "    - 다중 분류를 이진 분류로 근사하는 것이 네거티브 샘플링을 이해하는 데 중요한 개념\n",
    "    - 즉 target 단어에 해당하는 index의 값만 확률로 구하는 것이 목표\n",
    "    [그림 4-2-2]\n",
    "    \n",
    "    4.2.3 시그모이드 함수와 교차 엔트로피 오차\n",
    "    - 마지막 출력층 값을 확률로 바꾸기 위해 `sigmoid` 함수 활용 : y = 1 / (1 + exp(-x))\n",
    "    - [식 4.3] L = -(t log y + (1-t) log (1-y)) \n",
    "    - [warning] 책에서는 [식 1.7] L = -(시그마_k(t_k x log(y_k)) 와 위의 [식 4.3]이 다중 분류에서 출력층에 뉴런을 2개만 사용할 경우 위의 식과 같아진다고 하는데 \n",
    "        - 식 1.7의 경우 k 번째 정답 값이 0인경우 0 x log(y_k) = 0이 되므로 0인 라벨의 loss는 더하지 않게 되지만\n",
    "        - 식 4.3의 경우 t 가 0이면 log(1-y) 값이 loss에 들어간다\n",
    "        따라서 두 경우는 같아질 수 없지 않을까?\n",
    "    - [식 4.4] 출력층의 backward의 경우 L을 x(sigmoid 전)로 미분한 값, *y-t*만 넘겨주면 된다. (매우 simple!)\n",
    "    4.2.4 다중 분류에서 이진 분류로 구현\n",
    "    - class Embedding Dot\n",
    "                                   \n",
    "    4.2.5 네거티브 샘플링\n",
    "    - 4.2.4 까지는 target, 즉 정답인 단어에 해당하는 Loss만 구하게 된다. 그렇다면 정답이 아닌 다른 단어에 대한 확률값은 어떻게 구할지 잘 학습하지 못한다.\n",
    "    - 정답이 아닌 단어는 낮은 확률을 예측할 수 있게 학습하도록 부정적인 예, negative sample 몇 가지를 더 넣어주자.\n",
    "    - 예를 들어, you say goodbye and i hello . 에서 you 와 goodbye가 input으로 들어갔다면, 그에 대한 답은 [0, 1, 0, 0, ..  ,0] 이 되어야 한다. 따라서 target index, say의 index는 1로 예측해야하고, 나머지 index는 0으로 예측해야한다는 뜻. target = [0,1,0,..,0] 에서 정답 index는 1이고 나머지 0,2,3,4,5,...,vocab_size 는 모두 0으로 예측해야한다. 이때, 3,4,5 등을 부정적인 예로 추가해 출력층을 구성하고 그 출력층에 대한 loss를 계산하고 역전파한다면 몇 가지 부정적인 예시에 대해서도 잘 학습할 수 있다. \n",
    "                                   \n",
    "    4.2.6 네거티브 샘플링의 샘플링 기법\n",
    "    - 그렇다면 부정적인 예를 어떤 기준으로 sampling 할 것인가?\n",
    "    - 그에 대한 답은, 말뭉치의 단어별 출현 횟수를 바탕으로 확률 분포를 구한다. : `UnigramSampler(corpus, power, sample_size)`\n",
    "    - 이때, 출현 빈도가 낮은 단어의 선택을 높여주기 위해 확률 분포에서 구한 값들 0.75 제곱하고 해당 확률 값을 다시 구한다. 즉, 출현 빈도가 낮은 단어의 확률 값을 높여주고, 다른 확률 값은 상대적으로 낮출 수 있게 되어 비교적 골고루 단어가 선택되도록 하는 것이 목적이다.\n",
    "    \n",
    "    4.2.7 네거티브 샘플링 구현\n",
    "    - class NagetiveSamplingLoss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# 4.2.3 sigmoid with loss 구현\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx\n",
    "\n",
    "# 4.2.4 다중 분류에서 이진 분류로 구현\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n",
    "\n",
    "# 4.2.7 네거티브 샘플링 구현\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = np.asnumpy(target[i])\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n",
    "            # 부정적 예에 타깃이 포함될 수 있다.\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample\n",
    "\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 긍정적 예 순전파\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 부정적 예 순전파\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 개선판 word2vec 학습\n",
    "    4.3.1 CBOW 모델 구현\n",
    "    4.3.2 CBOW 모델 학습 코드\n",
    "    4.3.3 CBOW 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.1 모델\n",
    "import sys\n",
    "sys.path.append('D:/ANACONDA/envs/tf-gpu/code/NLP')\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size,hidden_size,window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V,H).astype('f')\n",
    "        W_out = 0.01 * np.random.rand(V,H).astype('f')\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out,corpus, power=0.75, sample_size=5)\n",
    "        \n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        #인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "    \n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:,i])\n",
    "        h *= 1/ len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h,target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None\n",
    "    \n",
    "# Skip-Gram\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * rn(V, H).astype('f')\n",
    "        W_out = 0.01 * rn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer = Embedding(W_in)\n",
    "        self.loss_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "            self.loss_layers.append(layer)\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer] + self.loss_layers\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "\n",
    "        loss = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            loss += layer.forward(h, contexts[:, i])\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for i, layer in enumerate(self.loss_layers):\n",
    "            dh += layer.backward(dout)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 함수\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.util import clip_grads\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=500):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('..')\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "import pickle\n",
    "#from common.trainer import Trainer\n",
    "#from common.optimizer import Adam\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "#from cbow import CBOW\n",
    "#from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "\n",
    "def to_cpu(x):\n",
    "    import numpy\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return np.asnumpy(x)\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 손실 4.16\n",
      "| 에폭 1 |  반복 3001 / 9295 | 시간 26[s] | 손실 2.64\n",
      "| 에폭 1 |  반복 6001 / 9295 | 시간 52[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 9001 / 9295 | 시간 78[s] | 손실 2.31\n",
      "| 에폭 2 |  반복 1 / 9295 | 시간 81[s] | 손실 2.27\n",
      "| 에폭 2 |  반복 3001 / 9295 | 시간 107[s] | 손실 2.20\n",
      "| 에폭 2 |  반복 6001 / 9295 | 시간 133[s] | 손실 2.14\n",
      "| 에폭 2 |  반복 9001 / 9295 | 시간 159[s] | 손실 2.09\n",
      "| 에폭 3 |  반복 1 / 9295 | 시간 162[s] | 손실 2.08\n",
      "| 에폭 3 |  반복 3001 / 9295 | 시간 188[s] | 손실 2.00\n",
      "| 에폭 3 |  반복 6001 / 9295 | 시간 214[s] | 손실 1.98\n",
      "| 에폭 3 |  반복 9001 / 9295 | 시간 240[s] | 손실 1.95\n",
      "| 에폭 4 |  반복 1 / 9295 | 시간 243[s] | 손실 1.94\n",
      "| 에폭 4 |  반복 3001 / 9295 | 시간 269[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 6001 / 9295 | 시간 295[s] | 손실 1.87\n",
      "| 에폭 4 |  반복 9001 / 9295 | 시간 321[s] | 손실 1.86\n",
      "| 에폭 5 |  반복 1 / 9295 | 시간 324[s] | 손실 1.85\n",
      "| 에폭 5 |  반복 3001 / 9295 | 시간 350[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 6001 / 9295 | 시간 376[s] | 손실 1.78\n",
      "| 에폭 5 |  반복 9001 / 9295 | 시간 402[s] | 손실 1.77\n",
      "| 에폭 6 |  반복 1 / 9295 | 시간 404[s] | 손실 1.78\n",
      "| 에폭 6 |  반복 3001 / 9295 | 시간 430[s] | 손실 1.70\n",
      "| 에폭 6 |  반복 6001 / 9295 | 시간 456[s] | 손실 1.71\n",
      "| 에폭 6 |  반복 9001 / 9295 | 시간 483[s] | 손실 1.71\n",
      "| 에폭 7 |  반복 1 / 9295 | 시간 485[s] | 손실 1.71\n",
      "| 에폭 7 |  반복 3001 / 9295 | 시간 511[s] | 손실 1.63\n",
      "| 에폭 7 |  반복 6001 / 9295 | 시간 538[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 9001 / 9295 | 시간 564[s] | 손실 1.65\n",
      "| 에폭 8 |  반복 1 / 9295 | 시간 566[s] | 손실 1.65\n",
      "| 에폭 8 |  반복 3001 / 9295 | 시간 592[s] | 손실 1.58\n",
      "| 에폭 8 |  반복 6001 / 9295 | 시간 619[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 9001 / 9295 | 시간 646[s] | 손실 1.59\n",
      "| 에폭 9 |  반복 1 / 9295 | 시간 648[s] | 손실 1.59\n",
      "| 에폭 9 |  반복 3001 / 9295 | 시간 675[s] | 손실 1.53\n",
      "| 에폭 9 |  반복 6001 / 9295 | 시간 701[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 9001 / 9295 | 시간 727[s] | 손실 1.55\n",
      "| 에폭 10 |  반복 1 / 9295 | 시간 730[s] | 손실 1.56\n",
      "| 에폭 10 |  반복 3001 / 9295 | 시간 756[s] | 손실 1.48\n",
      "| 에폭 10 |  반복 6001 / 9295 | 시간 782[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 9001 / 9295 | 시간 809[s] | 손실 1.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.3.2 학습\n",
    "\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기 + target, contexts 만들기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "    \n",
    "# 모델 등 생성 - CBOW or SkipGram\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "\n",
    "\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size, eval_interval = 3000) # eval_interval=500\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "word_vecs = model.word_vecs\n",
    "if GPU :\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params1.pkl'\n",
    "with open(pkl_file,'wb') as f:\n",
    "    pickle.dump(params,f,-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.3 CBOW 모델 평가\n",
    "# GPU -> CPU, cupy -> numpy 로\n",
    "#from common.util import most_similar, analogy\n",
    "import numpy as np\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "        \n",
    "        \n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.7529296875\n",
      " i: 0.724609375\n",
      " your: 0.623046875\n",
      " someone: 0.60693359375\n",
      " anybody: 0.60595703125\n",
      "\n",
      "[query] year\n",
      " month: 0.84033203125\n",
      " week: 0.76953125\n",
      " spring: 0.75634765625\n",
      " summer: 0.73681640625\n",
      " decade: 0.7021484375\n",
      "\n",
      "[query] car\n",
      " window: 0.61376953125\n",
      " truck: 0.59765625\n",
      " luxury: 0.59326171875\n",
      " auto: 0.58984375\n",
      " cars: 0.5625\n",
      "\n",
      "[query] toyota\n",
      " nissan: 0.6650390625\n",
      " nec: 0.64697265625\n",
      " honda: 0.64501953125\n",
      " minicomputers: 0.630859375\n",
      " ibm: 0.6279296875\n"
     ]
    }
   ],
   "source": [
    "pkl_file = './cbow_params1.pkl'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    \n",
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']\n",
    "id_to_word = params['id_to_word']\n",
    "\n",
    "# 가장 비슷한(most similar) 단어 뽑기\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "[analogy] king:man = queen:?\n",
      " a.m: 6.46875\n",
      " woman: 5.25390625\n",
      " father: 4.7265625\n",
      " daffynition: 4.70703125\n",
      " toxin: 4.61328125\n",
      "\n",
      "[analogy] take:took = go:?\n",
      " 're: 4.33203125\n",
      " went: 4.1796875\n",
      " came: 4.17578125\n",
      " were: 3.966796875\n",
      " are: 3.89453125\n",
      "\n",
      "[analogy] car:cars = child:?\n",
      " a.m: 6.93359375\n",
      " rape: 5.96484375\n",
      " daffynition: 5.8046875\n",
      " children: 5.3125\n",
      " incest: 5.3046875\n",
      "\n",
      "[analogy] good:better = bad:?\n",
      " more: 5.72265625\n",
      " less: 5.453125\n",
      " rather: 5.37890625\n",
      " greater: 4.5703125\n",
      " faster: 4.2265625\n"
     ]
    }
   ],
   "source": [
    "# 유추(analogy) 작업\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 정리\n",
    "    - Embedding 계층은 단어의 분산 표현을 담고 있다\n",
    "    - word2vec의 개선을 위해 다음 2가지 작업을 수행했다.\n",
    "        - Embedding 계층에서 특정 단어의 index만 뽑아 계산하도록\n",
    "        - Negative sampling 을 통해 다중 분류를 이진 분류로, 몇 가지의 단어들의 확률값과 Loss를 계산하도록\n",
    "    - word2vec의 Embedding, 분산 표현에는 단어의 의미가 들어가 있고 비슷한 맥락에서 사용되는 단어는 Embedding 공간에서 서로 가까이 위치한다.\n",
    "    - word2vec의 Embedding, 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈 문제로 풀 수 있다.\n",
    "    - 전이 학습 측면에서 특히 중요하며, 단어의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
